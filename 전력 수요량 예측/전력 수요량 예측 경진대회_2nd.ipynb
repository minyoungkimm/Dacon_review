{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================<Coding 구성>=======================================\n",
    "# [Local Function #1]: SAMPE calculation module\n",
    "# [Local Function #2]: AR_data_set calculation module\n",
    "# [Local Function #3]: AR_day_set calculation module\n",
    "# [Local Function #4]: linear_prediction calculation module\n",
    "# [Local Function #5]: Random forest module\n",
    "# [Local Function #6]: DNN module\n",
    "\n",
    "# [Main function]\n",
    "# [Section #1]: Data loading section\n",
    "# [Section #2]: Data generation for training set\n",
    "# [Section #3]: Anormaly detection using AR model \n",
    "# [Section #4]: data prediction for hour profile\n",
    "# [Section #5]: data prediction for day profile\n",
    "# [Section #6]: data prediction for month profile\n",
    "#=============================================================================\n",
    "\n",
    "import pandas as pd             #데이터 전처리\n",
    "import numpy as np              #데이터 전처리\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function <SAMPE calculation module>\n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# <A> : Real data.\n",
    "# <F> : Forecasting data.\n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# smape result.\n",
    "#------------------------------------------------------------------------------ \n",
    "\n",
    "def smape(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function <_data_set calculation module>\n",
    "# 시간 데이터 예측을 위한 데이터 셋 추출 함수.\n",
    "# test.csv 파일 내에 전력 데이터를 요일 타입을 고려하여 분류함.\n",
    "# 전날 데이터를 학습하여 다음날을 예측하는 방식을 사용하며, 요일 타입은 2가지로 \n",
    "# 분류함. 월~금은 Workday, 토~일은 weekend    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# <Data> : test.csv.\n",
    "# <place_id>: power meter ID.\n",
    "# <prev_type>: 예측 전 날의 데이터 타입(<1>:Workday <2>:Weekend)\n",
    "# <Curr_type>: 예측 날의 데이터 타입(<1>:Workday <2>:Weekend)  \n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# <TrainAR>: 예측 전날의 data set [day x time]\n",
    "# <TestAR>: 예측 날의 data set [day x time]\n",
    "#------------------------------------------------------------------------------ \n",
    "def AR_data_set(Data, place_id, prev_type, Curr_type):\n",
    "    \n",
    "    # Mon: 0 ~ Sun:6\n",
    "\n",
    "    TrainAR = []; TestAR = []\n",
    "    len_bad = 20  # 하루 내 NaN의 개수 기준, (24-len_bad)보다 많으면 그 날은 제거 \n",
    "    Power = Data[place_id].iloc  # test.csv에서 특정 id의 전력 데이터   \n",
    "    Date = Data[place_id].index  # test.csv에서 특정 id의 날짜 데이터 \n",
    "    prev_aloc = [0]*24;  curr_aloc = [0]*24  # pre-allocation\n",
    "    \n",
    "    for ii in range(24,len(Date)):\n",
    "        \n",
    "        \n",
    "        if (Date[ii].hour == 0) & (ii >48) & (np.sum(curr_aloc)!=24*curr_aloc[1])& (np.sum(prev_aloc)!=24*prev_aloc[1]):\n",
    "            prev_idx = 0;  curr_idx = 0      # bad data idx\n",
    "            \n",
    "            for kk in range(0,24):\n",
    "                if prev_aloc[kk] > -20:        # check the bad data.\n",
    "                    prev_idx =prev_idx+1     \n",
    "                else:                        # interpolate the bad data.\n",
    "                    # bad data일 경우, 앞뒤로 20개의 포인트를 가져와서 \n",
    "                    # interpolation 진행.\n",
    "                    temp = np.zeros([1,41])\n",
    "                    for qq in range(0,41):\n",
    "                        temp[0,qq] = Power[(ii-24)-(24-kk)-20+qq]\n",
    "                    \n",
    "                    temp_temp = pd.DataFrame(data = temp)\n",
    "\n",
    "                    temp = temp_temp.interpolate('spline',order =1)\n",
    "                    temp = temp.values\n",
    "                    prev_aloc[kk] = temp[0,20]\n",
    "                    \n",
    "\n",
    "            for kk in range(0,24):\n",
    "                if curr_aloc[kk]>-20:       # check the bad data.\n",
    "                    curr_idx =curr_idx+1\n",
    "                else:\n",
    "                    # bad data일 경우, 앞뒤로 20개의 포인트를 가져와서 \n",
    "                    # interpolation 진행.\n",
    "                    temp = np.zeros([1,41])\n",
    "                    for qq in range(0,41):\n",
    "                        temp[0,qq] = Power[(ii)-(24-kk)-20+qq]\n",
    "                    temp_temp = pd.DataFrame(data = temp)\n",
    "                    \n",
    "                    temp = temp_temp.interpolate('spline',order =1)\n",
    "                    temp = temp.values\n",
    "                    curr_aloc[kk] = temp[0,20]\n",
    "\n",
    "            # bad data가 특정 개수 이상이면, data set에 추가하지 않는다.  \n",
    "            if (prev_idx>len_bad)&(curr_idx>len_bad):\n",
    "                TrainAR.append(prev_aloc)\n",
    "                TestAR.append(curr_aloc)\n",
    "                        \n",
    "        # 0시에 하루 데이터 초기화.                     \n",
    "        if Date[ii].hour == 0:\n",
    "            prev_aloc = [0]*24\n",
    "            curr_aloc = [0]*24\n",
    "        \n",
    "        # 요일 데이터 확인.\n",
    "        prev_day = Date[ii-24].weekday()\n",
    "        curr_day = Date[ii].weekday()\n",
    "        \n",
    "        # 요일 데이터 타입 분류\n",
    "        # Workday(1) = day type&lt;5(월~금)\n",
    "        # Workday(2) = day type&gt;4(토~일)\n",
    "        if ((prev_type ==1)&(prev_day<5))&((Curr_type ==2)&(curr_day>4)):           \n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24]\n",
    "            curr_aloc[Date[ii].hour] = Power[ii]\n",
    "        \n",
    "        if ((prev_type ==1)&(prev_day<5))&((Curr_type ==1)&(curr_day<5)):           \n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24]\n",
    "            curr_aloc[Date[ii].hour] = Power[ii]\n",
    "            \n",
    "        if ((prev_type ==2)&(prev_day>4))&((Curr_type ==2)&(curr_day>4)):           \n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24]\n",
    "            curr_aloc[Date[ii].hour] = Power[ii]\n",
    "            \n",
    "        if ((prev_type ==2)&(prev_day>4))&((Curr_type ==1)&(curr_day<5)):           \n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24]\n",
    "            curr_aloc[Date[ii].hour] = Power[ii]\n",
    "                \n",
    "    TrainAR = np.array(TrainAR)\n",
    "    TestAR = np.array(TestAR)\n",
    "    return TrainAR, TestAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function &lt;AR_day_set calculation module&gt;\n",
    "# 하루 사용 데이터 예측을 위한 데이터 셋 추출 함수.\n",
    "# test.csv 파일 내에 전력 데이터를 요일 타입을 고려하여 분류함.\n",
    "# &lt;요일 타입&gt;: 월 ~ 일\n",
    "# Similar day approach method만을 활용할 예정이기 때문에 최근 데이터 6주만을 \n",
    "# 정리함.    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# &lt;Data&gt; : test.csv.\n",
    "# &lt;place_id&gt;: power meter ID.\n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;temp_day&gt;: 과거 하루 전력 사용량  [week number(최근 순) x day type]\n",
    "#------------------------------------------------------------------------------     \n",
    "def AR_day_set(data, place_id):\n",
    "\n",
    "    Power = data[place_id].values       #전력 데이터 \n",
    "    Date = data[place_id].index         #요일 데이터 \n",
    "    \n",
    "    temp_day = np.zeros([6, 7])         # pre-allocation for output dataset\n",
    "    mon_idx = np.zeros([1, 7])          # 몇 번째 week인지 확인하는 idx\n",
    "    \n",
    "    for ii in range(0,len(Power)-500):\n",
    "                \n",
    "        idx = len(Power) - ii -1\n",
    "        \n",
    "        day_idx = Date[idx].weekday()       # data의 요일정보\n",
    "        time_idx = Date[idx].hour           # data의 시간정보 \n",
    "\n",
    "        if mon_idx[0, day_idx] < 6:         # 6번째 week 이상이면 추가 X\n",
    "            \n",
    "            if np.isnan(Power[idx]):        # bad data restortion\n",
    "                res_data = np.zeros([1, 9]) \n",
    "                # 1주전, 2주전, 3주전의 같은 요일, 시간 데이터를 저장 후 mean\n",
    "                res_data[0,0] = Power[idx-24*7-1]\n",
    "                res_data[0,1] = Power[idx-24*7]\n",
    "                res_data[0,2] = Power[idx-24*7+1]\n",
    "                \n",
    "                res_data[0,3] = Power[idx-48*7-1]\n",
    "                res_data[0,4] = Power[idx-48*7]\n",
    "                res_data[0,5] = Power[idx-48*7+1]\n",
    "                \n",
    "                res_data[0,6] = Power[idx-1]\n",
    "                res_data[0,7] = Power[idx-3*24*7]\n",
    "                res_data[0,8] = Power[idx+1]\n",
    "                # 하루 사용량 저장을 위한 시간 데이터 합\n",
    "                temp_day[int(round(mon_idx[0,day_idx])), day_idx] = temp_day[int(round(mon_idx[0,day_idx])), day_idx]+ np.nanmean(res_data)\n",
    "                \n",
    "            else:\n",
    "                # 하루 사용량 저장을 위한 시간 데이터 합\n",
    "                temp_day[int(round(mon_idx[0,day_idx])), day_idx] = temp_day[int(round(mon_idx[0,day_idx])), day_idx] + Power[idx]\n",
    "\n",
    "                \n",
    "            if time_idx == 0:\n",
    "                # 요일이 지나면, week확인 idx +1\n",
    "                mon_idx[0,day_idx] = mon_idx[0,day_idx] + 1\n",
    "                \n",
    "    return temp_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function &lt;linear_prediction calculation module&gt;\n",
    "# 선형 예측 방식 구현. (Autoregressive model)\n",
    "# Y(예측 날) = A(coefficient)*X(예측전날)\n",
    "# X-1(역행렬)*Y = A\n",
    "# A를 추출하여, 예측에 활용함.     \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# &lt;trainAR&gt;: test.csv의 예측 전날 데이터 셋 \n",
    "# &lt;testAR&gt;: test.csv의 예측 날 데이터 셋 \n",
    "# &lt;flen&gt;: filter length(예측전날의 몇개의 데이터를 가져다 쓸지 결정)\n",
    "# &lt;test_data&gt;: 실제로 예측 하고 싶은 전날의 데이터(TEST 데이터)    \n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;avr_smape&gt;: Training set으로 테스트 했을 때, smape\n",
    "# &lt;fcst&gt;: test_data의 예측 결과 \n",
    "# &lt;pred&gt;: Traing set으로 예측했던 예측 결과    \n",
    "#------------------------------------------------------------------------------        \n",
    "def linear_prediction(trainAR, testAR, flen, test_data):\n",
    "    \n",
    "    len_tr = len(trainAR[0,:])   # 시간 포인트 수 \n",
    "    day_t = len(trainAR)\n",
    "    pred = np.empty((len(trainAR),len_tr))\n",
    "    fcst = np.empty((len(trainAR),len_tr))\n",
    "    \n",
    "    for j in range(0, day_t):\n",
    "        if day_t>1:\n",
    "            x_ar=np.delete(trainAR[:,len_tr-flen:len_tr], (j), axis=0)\n",
    "            y=np.delete(testAR, (j), axis=0)\n",
    "        else:\n",
    "            x_ar = trainAR[:,len_tr-flen:len_tr]\n",
    "            y = testAR\n",
    "            \n",
    "        pi_x_ar = np.linalg.pinv(x_ar)\n",
    "        lpc_c = np.empty((len(x_ar),flen))\n",
    "\n",
    "        \n",
    "        lpc_c=np.matmul(pi_x_ar, y)\n",
    "        \n",
    "        test_e = trainAR[j,:]\n",
    "        test_ex = test_e[len_tr-flen:len_tr]\n",
    "        pred[j,:]=np.matmul(test_ex, lpc_c)  \n",
    "        \n",
    "    \n",
    "    x_ar = trainAR[:,len_tr-flen:len_tr]\n",
    "    y = testAR\n",
    "    pi_x_ar = np.linalg.pinv(x_ar)\n",
    "    lpc_c = np.empty((len(x_ar),flen))\n",
    "\n",
    "        \n",
    "    lpc_c=np.matmul(pi_x_ar, y)\n",
    "    \n",
    "        \n",
    "    Test_AR = testAR[0:len(testAR),:]\n",
    "        \n",
    "    smape_list=np.zeros((len(pred),1))\n",
    "\n",
    "    for i in range(0,len(pred)):\n",
    "        smape_list[i]=smape(pred[i,:], Test_AR[i,:])\n",
    "        avr_smape = np.mean(smape_list)  \n",
    "    \n",
    "    test_e = test_data\n",
    "    test_ex = test_e[len_tr-flen:len_tr]   \n",
    "    fcst = np.matmul(test_ex,lpc_c)\n",
    "\n",
    "    return avr_smape, fcst, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function &lt;Similar day approach module&gt;\n",
    "# Similar day approaach method 구현.\n",
    "# 같은 요일 타입의 날의 데이터를 N개를 추출하여 평균을 취하여 사용함.    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# &lt;trainAR&gt;: test.csv의 예측 전날 데이터 셋 \n",
    "# &lt;testAR&gt;: test.csv의 예측 날 데이터 셋 \n",
    "# &lt;slen&gt;: 추출한 날의 수 (N)\n",
    "# &lt;sim_set&gt;: 실제로 예측 하고 싶은 전날의 데이터(TEST 데이터)    \n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;simil_smape&gt;: Training set으로 테스트 했을 때, smape\n",
    "# &lt;simil_temp&gt;: test_data의 예측 결과   \n",
    "#------------------------------------------------------------------------------\n",
    "def similar_approach(trainAR, testAR, slen, sim_set):\n",
    "    simil_smape_list = np.zeros([1,len(testAR[:,0])])\n",
    "    \n",
    "    for col_ii in range(0,len(testAR[:,0])):    \n",
    "        simil_mean = []\n",
    "        simil_temp =np.zeros([1,24])\n",
    "        simil_idx =np.zeros([1,len(testAR[:,0])])\n",
    "        \n",
    "        for sub_col in range(0,len(testAR[:,0])):\n",
    "            simil_idx[0,sub_col] = smape(trainAR[col_ii,:],trainAR[sub_col,:])\n",
    "            \n",
    "        testAR_temp = np.delete(testAR, np.argmin(simil_idx), axis=0)\n",
    "        simil_idx = np.delete(simil_idx, np.argmin(simil_idx), axis=1)\n",
    "        \n",
    "        for search_len in range(0,slen):\n",
    "            simil_mean.append(testAR_temp[np.argmin(simil_idx), :])\n",
    "            testAR_temp = np.delete(testAR, np.argmin(simil_idx), axis=0)\n",
    "            simil_idx = np.delete(simil_idx, np.argmin(simil_idx), axis=1)\n",
    "              \n",
    "        for row_ii in range(0, 24):           \n",
    "            simil_temp[0, row_ii] = np.median(testAR_temp[:,row_ii])\n",
    "\n",
    "        simil_smape_list[0,col_ii] = smape(testAR[col_ii,:], simil_temp)\n",
    "        simil_smape  = np.mean(simil_smape_list)\n",
    "    \n",
    "    simil_mean = []\n",
    "    simil_temp =np.zeros([1,24])\n",
    "    simil_idx =np.zeros([1,len(testAR[:,0])])\n",
    "    testAR_temp = testAR\n",
    "    \n",
    "    for sub_col in range(0,len(testAR[:,0])):\n",
    "        simil_idx[0,sub_col] = smape(sim_set,trainAR[sub_col,:])\n",
    "\n",
    "    \n",
    "    for search_len in range(0,slen):\n",
    "        simil_mean.append(testAR_temp[np.argmin(simil_idx),:])\n",
    "        testAR_temp = np.delete(testAR, np.argmin(simil_idx), axis=0)\n",
    "        simil_idx = np.delete(simil_idx, np.argmin(simil_idx), axis=1)\n",
    "   \n",
    "    for row_ii in range(0, 24):           \n",
    "        simil_temp[0, row_ii] = np.median(testAR_temp[:,row_ii])\n",
    "    \n",
    "    return simil_smape, simil_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function &lt;Random forest module&gt;\n",
    "# Random forest를 이용한 regression 기반의 forecasting algorithm\n",
    "# 전날의 24시간 프로파일을 이용해 다음날 24시간의 프로파일을 예측하는 시스템.   \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# &lt;trainAR&gt;: test.csv의 예측 전날 데이터 셋 \n",
    "# &lt;testAR&gt;: test.csv의 예측 날 데이터 셋 \n",
    "# &lt;x_24hrs&gt;: 실제로 예측 하고 싶은 전날의 데이터(TEST 데이터)  \n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;ypr&gt;: x_24hrs를 이용한 예측 결과  \n",
    "# &lt;avr_smape&gt;: validation set으로 확인한, smape      \n",
    "#------------------------------------------------------------------------------\n",
    "def machine_learn_gen(trainAR, testAR, x_24hrs):\n",
    "    \n",
    "    Dnum=trainAR.shape[0]\n",
    "    lnum=trainAR.shape[1]\n",
    "    smape_list=np.zeros([Dnum,1])\n",
    "    \n",
    "    for ii in range(0,Dnum): # cross validation\n",
    "        trainAR_temp = np.delete(trainAR, ii, axis=0)\n",
    "        testAR_temp  = np.delete(testAR, ii, axis=0)\n",
    "        \n",
    "        # mae 기반의 loss를 이용한 randomforest model 생성\n",
    "        regr = RandomForestRegressor(max_depth=2, random_state=0,n_estimators=100, criterion='mae')\n",
    "        regr.fit(trainAR_temp, testAR_temp)\n",
    "        \n",
    "        \n",
    "        x_temp = np.zeros([1,lnum])\n",
    "        for kk in range(0,lnum):\n",
    "            x_temp[0,kk] = trainAR[ii, kk]\n",
    "            \n",
    "        ypr = regr.predict(x_temp)\n",
    "\n",
    "        yre = np.zeros([1,lnum])\n",
    "        for kk in range(0,lnum):\n",
    "            yre[0,kk] = testAR[ii, kk]\n",
    "        \n",
    "        smape_list[ii] = smape(np.transpose(ypr),np.transpose(yre))\n",
    "        \n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0,n_estimators=100, criterion='mae')\n",
    "    regr.fit(trainAR, testAR)\n",
    "        \n",
    "    x_24hrs = np.reshape(x_24hrs,(-1,lnum))\n",
    "    \n",
    "    avr_smape = np.mean(smape_list)\n",
    "    ypr=regr.predict(x_24hrs)\n",
    "    \n",
    "    return ypr,  avr_smape, smape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function &lt;DNN module&gt;\n",
    "# DNN를 이용한 regression 기반의 forecasting algorithm\n",
    "# 전날의 24시간 프로파일을 이용해 \n",
    "# 다음날 24시간의 프로파일을 예측할 수 있는 DNN 모델 생성.    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# &lt;trainAR&gt;: test.csv의 예측 전날 데이터 셋 \n",
    "# &lt;testAR&gt;: test.csv의 예측 날 데이터 셋 \n",
    "# &lt;EPOCHS&gt;: DNN의 학습을 위한 epoch size  \n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;model&gt;: 학습된 모델 출력  \n",
    "# &lt;avr_smape&gt;: test set으로 테스트 했을 때의 평균, smape  \n",
    "# &lt;smape_list&gt;: test set으로 테스트 했을 때의 각각, smape      \n",
    "#------------------------------------------------------------------------------     \n",
    "def non_linear_model_gen(trainAR, testAR, EPOCHS):\n",
    "    \n",
    "    numData=np.size(trainAR,0)\n",
    "    numTr=int(numData*0.8)\n",
    "    Xtr=trainAR[0:numTr-1,:]\n",
    "    Ytr=testAR[0:numTr-1,:]\n",
    "    \n",
    "    Xte=trainAR[numTr:numData,:]\n",
    "    Yte=testAR[numTr:numData,:]\n",
    "    \n",
    "    num_tr = np.size(trainAR,1)\n",
    "    num_te = np.size(testAR,1)\n",
    "    \n",
    "    def build_model():\n",
    "        model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(num_tr,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_te)\n",
    "        ])\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "        model.compile(loss='mae',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['mae', 'mse'])\n",
    "        return model\n",
    "\n",
    "    model = build_model()\n",
    "#    model.summary()\n",
    "    \n",
    "    #example_batch = Xtr[:10]\n",
    "    #example_result = model.predict(example_batch)\n",
    "    #example_result\n",
    "    \n",
    "    class PrintDot(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            if epoch % 100 == 0: print('')\n",
    "    \n",
    "    \n",
    "    history = model.fit(\n",
    "      Xtr, Ytr,\n",
    "      epochs=EPOCHS, verbose=0,\n",
    "      callbacks=[PrintDot()])\n",
    "   \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    hist.tail()\n",
    "    \n",
    "    Ypr = model.predict(Xte)\n",
    "    \n",
    "    smape_list=np.zeros((len(Ypr),1))\n",
    "    \n",
    "    for i in range(0,len(Ypr)):\n",
    "        smape_list[i]=smape(Ypr[i,:], Yte[i,:])\n",
    "    avr_smape=np.mean(smape_list)\n",
    "    \n",
    "    \n",
    "    return model, avr_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ==============================&lt;Main&gt;=======================================\n",
    "#%% Section #1: Loading data...    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;test&gt; : test.csv\n",
    "# &lt;submission&gt;: Data frame for submission.\n",
    "#------------------------------------------------------------------------------ \n",
    "os.chdir('data')                    # Changing Dir. (&lt;main folder&gt;/data)\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('submission_1002.csv')\n",
    "os.chdir('..')                      # Changing Dir. (&lt;main folder&gt;)\n",
    "\n",
    "test['Time'] = pd.to_datetime(test.Time)\n",
    "test = test.set_index('Time')\n",
    "\n",
    "print('Section [1]: Loading data...............')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Section #2: Data generation for training set  \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# &lt;test&gt; : Test set\n",
    "# &lt;submission&gt;: Data frame for submission.\n",
    "# &lt;prev_type&gt;: Day type of the previous day (workday = 1, weekend =2).\n",
    "# &lt;curr_type&gt;: Day type of the current day (workday = 1, weekend =2).\n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;testAR&gt; : Test set\n",
    "# &lt;trainAR&gt;: Training set\n",
    "# &lt;subm_24hrs&gt;: test data for 24hrs prediction\n",
    "#------------------------------------------------------------------------------ \n",
    "agg = {}\n",
    "comp_smape =[]\n",
    "key_idx = 0\n",
    "\n",
    "for key in test.columns:\n",
    "    key_idx = key_idx + 1\n",
    "    print([key,key_idx])\n",
    "    prev_type = 2   # 전날 요일 타입\n",
    "    curr_type = 2   # 예측날 요일 타입\n",
    "    trainAR, testAR = AR_data_set(test, key, prev_type, curr_type)\n",
    "    \n",
    "    print('Section [2]: Data generation for training set...............')\n",
    "\n",
    "    # [시간 예측을 위한 마지막 24pnt 추출]\n",
    "    # NaN 값처리를 위해서 마지막 40pnts 추출 한 후에 \n",
    "    # interpolation하고 나서 24pnts 재추출 \n",
    "    temp_test = test[key].iloc[8759-40:]      \n",
    "    temp_test = temp_test.interpolate(method='spline', order=2)\n",
    "    \n",
    "    temp_test = np.array(temp_test.values)\n",
    "    temp_test = temp_test[len(temp_test)-24:len(temp_test)+1]\n",
    "    subm_24hrs = temp_test\n",
    "    \n",
    "    del temp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Section #3: Anormaly detection using AR model  \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# &lt;testAR&gt; : Test set\n",
    "# &lt;trainAR&gt;: Training set\n",
    "#------------------------------------------------------------------------------ \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;testAR&gt; : Test set\n",
    "# &lt;trainAR&gt;: Training set\n",
    "#------------------------------------------------------------------------------   \n",
    "    fchk = 1        # filter length\n",
    "    temp_idx = []\n",
    "    smape_lin = []\n",
    "    \n",
    "    # 한 행씩 linear prediction을 테스트해보고 NaN이 발견된다면, 그 행을 제거.\n",
    "    for chk_bad in range(0, len(trainAR[:,0])):\n",
    "        prev_smape = 200 # SMAPE 기준값 \n",
    "        nan_chk = 0      # NaN chk idx\n",
    "        \n",
    "        trainAR_temp = np.zeros([1,24])     # pre-allocation\n",
    "        testAR_temp = np.zeros([1,24])      # pre-allocation\n",
    "        \n",
    "        # 한 행씩 테스트를 하기 위한 변수 설정\n",
    "        for ii in range(0,24):\n",
    "            trainAR_temp[0,ii] = trainAR[chk_bad,ii]\n",
    "            testAR_temp[0,ii] = testAR[chk_bad,ii]\n",
    "        \n",
    "        # linear prediction test\n",
    "        lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR_temp, testAR_temp, fchk, subm_24hrs)\n",
    "            \n",
    "        if np.isnan(lin_sampe):     # SMAPE가 NaN 경우, 그 행을 제거\n",
    "            nan_chk = 1\n",
    "        if np.isnan(np.sum(trainAR_temp)): # chk_bad의 행이 NaN을 포함할 경우 제거\n",
    "            nan_chk = 1         \n",
    "        if np.isnan(np.sum(testAR_temp)): # chk_bad의 행이 NaN을 포함할 경우 제거\n",
    "            nan_chk = 1\n",
    "                \n",
    "        if nan_chk == 1: #NaN 값이 있는 행 넘버를 append\n",
    "            temp_idx.append(chk_bad)\n",
    "    \n",
    "    # NaN 값이 나타난 data set은 제거 \n",
    "    trainAR = np.delete(trainAR, temp_idx, axis=0)\n",
    "    testAR = np.delete(testAR, temp_idx, axis=0)                 \n",
    "        \n",
    "    del_smape = np.zeros([1,len(trainAR[:,1])])\n",
    "    prev_smape = 200\n",
    "    fchk = 0\n",
    "    \n",
    "    # filter length 최적화 \n",
    "    for chk in range(3,24):\n",
    "        # filter length을 바꿔가며 Smape가 최소가 되는 값을 찾아감.\n",
    "        lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR, testAR, chk, subm_24hrs)\n",
    "        if prev_smape&gt;lin_sampe:\n",
    "            fchk = chk\n",
    "            prev_smape = lin_sampe    \n",
    "   \n",
    "    # 필요없는 데이터 제거\n",
    "    # 한 줄(하루)씩 제거해가면서 SMAPE 결과를 분석.\n",
    "    for chk_lin in range(0,len(trainAR[:,1])):\n",
    "        \n",
    "        trainAR_temp = np.delete(trainAR, chk_lin, axis=0)\n",
    "        testAR_temp = np.delete(testAR, chk_lin, axis=0)\n",
    "        lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR_temp, testAR_temp, fchk, subm_24hrs)          \n",
    "         \n",
    "        del_smape[0,chk_lin] = lin_sampe\n",
    "    \n",
    "    # SMAPE에 악영향을 주는 행을 제거      \n",
    "    trainAR = np.delete(trainAR, np.argmin(del_smape), axis=0)\n",
    "    testAR = np.delete(testAR, np.argmin(del_smape), axis=0)\n",
    "    del_smape = np.delete(del_smape, np.argmin(del_smape), axis =1)\n",
    "\n",
    "    print('Section [3]: mitigating bad data...............')\n",
    "    \n",
    "    del nan_chk, lin_sampe, fcst_temp, pred_hyb, prev_smape, temp_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Section #4: Prediction test\n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;fcst&gt; : Predicted hour profile result \n",
    "#------------------------------------------------------------------------------ \n",
    "    \n",
    "    # DNN model \n",
    "    EPOCHS = 80\n",
    "    Non_NNmodel, non_smape = non_linear_model_gen(trainAR, testAR, EPOCHS)\n",
    "    \n",
    "    # random forest model\n",
    "    mac_fcst, Mac_smape, smape_listss = machine_learn_gen(trainAR, testAR,subm_24hrs)\n",
    "    \n",
    "    # linear model\n",
    "    lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR, testAR, fchk, subm_24hrs)\n",
    "    \n",
    "    # Similar day approach model\n",
    "    temp_24hrs = np.zeros([1,24])    # np.array type으로 변경. \n",
    "    for qq in range(0,24):\n",
    "        temp_24hrs[0,qq] = subm_24hrs[qq]\n",
    "    \n",
    "    # Similar day approach model 최적화 (몇 개의 날(N)을 가져오는 게 좋은 지 평가.)    \n",
    "    prev_smape = 200\n",
    "    fsim = 0  # N개의 날 \n",
    "    for sim_len in range(2, 5):    \n",
    "        sim_smape,  fcst_sim = similar_approach(trainAR, testAR, sim_len, temp_24hrs)\n",
    "        if prev_smape>sim_smape:\n",
    "            fsim = sim_len\n",
    "            prev_smape = sim_smape\n",
    "            \n",
    "    # Similar day approach model       \n",
    "    sim_smape,  fcst_sim = similar_approach(trainAR, testAR, fsim, temp_24hrs)\n",
    "    # ---------------------------------------------------------------------------------------    \n",
    "    \n",
    "    minor_idx = 0 # Autoregression model에서 minor value가 나타나면, \n",
    "    # 모델을 Autoregression model에서 similar day appreach로 변경 진행.\n",
    "    \n",
    "    # SMAPE가 linear model이 가장 작으면, 해당 결과 사용\n",
    "    if (lin_sampe<non_smape)&(lin_sampe<Mac_smape)&(lin_sampe<sim_smape):    \n",
    "        fcst = np.zeros([1,24])  \n",
    "        for qq in range(0,24):\n",
    "            fcst[0,qq] = fcst_temp[qq]\n",
    "            \n",
    "            if fcst_temp[qq]&lt;0:\n",
    "                minor_idx = minor_idx+1\n",
    "                \n",
    "    # SMAPE가 DNN model이 가장 작으면, 해당 결과 사용\n",
    "    if (non_smape<lin_sampe)&(non_smape<Mac_smape)&(non_smape<sim_smape):\n",
    "        temp_24hrs = np.zeros([1,24])\n",
    "        for qq in range(0,24):\n",
    "            temp_24hrs[0,qq] = subm_24hrs[qq]\n",
    "            \n",
    "        fcst = Non_NNmodel.predict(temp_24hrs)\n",
    "    \n",
    "    # SMAPE가 random forest model이 가장 작으면, 해당 결과 사용\n",
    "    if (Mac_smape<non_smape)&(Mac_smape<lin_sampe)&(Mac_smape<sim_smape):\n",
    "        fcst = mac_fcst\n",
    "    \n",
    "    # SMAPE가 Similar day approach model이 가장 작으면, 해당 결과 사용        \n",
    "    if (sim_smape<non_smape)&(sim_smape<lin_sampe)&(sim_smape<Mac_smape):\n",
    "        fcst = fcst_sim\n",
    "        \n",
    "    if (minor_idx>0):\n",
    "        fcst = fcst_sim\n",
    "    \n",
    "    # 각 SMAPE 결과 값을 정    \n",
    "    comp_smape.append([non_smape, lin_sampe,Mac_smape, sim_smape])\n",
    "\n",
    "    \n",
    "    a = pd.DataFrame() # a라는 데이터프레임에 예측값을 정리합니다.\n",
    "    \n",
    "    print('Section [4]: Hour prediction model...............')    \n",
    "    for i in range(24):\n",
    "        a['X2018_7_1_'+str(i+1)+'h']=[fcst[0][i]] # column명을 submission 형태에 맞게 지정합니다.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Section #5: Day prediction\n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;fcst_d&gt;: Predicted day profile (result)\n",
    "#------------------------------------------------------------------------------ \n",
    "    \n",
    "    fcst_d = np.zeros([1,10])       # pre-allocation of the result data\n",
    "    \n",
    "    \n",
    "    trainAR_Day = AR_day_set(test, key)  #데이터를 불러옵니다.\n",
    "    \n",
    "    # Similar day aprroach\n",
    "    day_idx = np.zeros([1, 10])\n",
    "    for ii in range(0, 10):\n",
    "        mod_idx = -1\n",
    "        temp_idx =  (ii+mod_idx) % 7     #예측하는 날에 맞는 요일 idx를 정리.\n",
    "        day_idx[0, ii]  = temp_idx\n",
    "        \n",
    "    \n",
    "    for ii in range(0,10):\n",
    "        flen = np.random.randint(3)+2    # 2~5개까지 랜덤하게 과거 데이터를 불러옵니다.\n",
    "    \n",
    "        temp_day = np.zeros([1, flen])\n",
    "        \n",
    "        for jj in range(0, flen):  \n",
    "            temp_day[0,jj] =  trainAR_Day[jj, int(round(day_idx[0, ii]))]\n",
    "        \n",
    "        # 불러온 데이터를 평균을하여 예측함.\n",
    "        fcst_d[0,ii] = np.mean(temp_day)\n",
    "    \n",
    "    print('Section [5]: Day prediction model...............')\n",
    "    \n",
    "    for i in range(10):\n",
    "        a['X2018_7_'+str(i+1)+'_d']=[fcst_d[0][i]] # column명을 submission 형태에 맞게 지정합니다.\n",
    "    \n",
    "    del mod_idx, temp_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Section #6: Month prediction\n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# &lt;pred_(N)m&gt;: N번째 달의 전력 사용량 예측 결과\n",
    "#------------------------------------------------------------------------------ \n",
    "\n",
    "    mon_test = np.zeros([1,300])\n",
    "    \n",
    "    # Similar day aprroach \n",
    "    day_idx = np.zeros([1, 300])\n",
    "    for ii in range(0, 300):\n",
    "        mod_idx = -1\n",
    "        temp_idx = (ii+mod_idx) % 7  # 요일 idx 생성(월~일: 0~6)\n",
    "        day_idx[0, ii]  = temp_idx\n",
    "    \n",
    "    # 휴일의 경우, 일요일과 같은 데이터로 가정함.\n",
    "    day_idx[0,31+15-1] = 6 # 광복절\n",
    "    day_idx[0,31+31+24-1] = 6 # 추석\n",
    "    day_idx[0,31+31+25-1] = 6 # 추석\n",
    "    day_idx[0,31+31+26-1] = 6 # 대체휴일\n",
    "    day_idx[0,31+31+30+3-1] = 6 # 개천절\n",
    "    day_idx[0,31+31+30+9-1] = 6 # 한글날\n",
    "    day_idx[0,31+31+30+31+30+25-1] = 6 # 성탄절\n",
    "    \n",
    "   \n",
    "    for ii in range(0,300):\n",
    "        flen = np.random.randint(3)+1  # Similar day approach를 위한 1~4개의 데이터 추출\n",
    "    \n",
    "        temp_day = np.zeros([1, flen])\n",
    "        \n",
    "        for jj in range(0, flen):  \n",
    "             temp_day[0,jj] =  trainAR_Day[jj, int(round(day_idx[0, ii]))]\n",
    "            \n",
    "        mon_test[0,ii] = np.mean(temp_day)\n",
    "    \n",
    "   \n",
    "    # 결과 합         \n",
    "    pred_7m = np.sum(mon_test[0,0:31])\n",
    "    pred_8m = np.sum(mon_test[0,31:62])\n",
    "    pred_9m = np.sum(mon_test[0,62:92])\n",
    "    pred_10m = np.sum(mon_test[0,92:123])\n",
    "    pred_11m = np.sum(mon_test[0,123:153])\n",
    "    \n",
    "    a['X2018_7_m'] = [pred_7m] # 7월\n",
    "    a['X2018_8_m'] = [pred_8m] # 8월\n",
    "    a['X2018_9_m'] = [pred_9m] # 9월\n",
    "    a['X2018_10_m'] = [pred_10m] # 10월\n",
    "    a['X2018_11_m'] = [pred_11m] # 11월 \n",
    "  \n",
    "    a['meter_id'] = key \n",
    "    agg[key] = a[submission.columns.tolist()]  \n",
    "    \n",
    "    print('Section [6]: Month prediction model...............')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Section #6: Write the result data...\n",
    "# makes the result file\n",
    "# [File name]: dacon_submmision.csv\n",
    "\n",
    "os.chdir('data')  # Changing Dir. (&lt;main folder&gt;/data)\n",
    "output1 = pd.concat(agg, ignore_index=False)\n",
    "output2 = output1.reset_index().drop(['level_0','level_1'], axis=1)\n",
    "output2['id'] = output2['meter_id'].str.replace('X','').astype(int)\n",
    "output2 =  output2.sort_values(by='id', ascending=True).drop(['id'], axis=1).reset_index(drop=True)\n",
    "output2.to_csv('sub_baseline.csv', index=False)\n",
    "os.chdir('..')      # Changing Dir. (&lt;main folder&gt;)\n",
    "\n",
    "\n",
    "print('Section [7]: Saving the result files...............')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
